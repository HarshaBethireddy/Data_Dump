import pandas as pd
import re
import yaml
from z3 import *
import logging
from pathlib import Path
from typing import Dict, List, Tuple, Any, Optional, Set
import sys
from collections import defaultdict
import networkx as nx
from datetime import datetime
import io

# ==========================================
# CONFIGURATION LOADER
# ==========================================
class ConfigLoader:
    """Loads project-specific configurations from YAML"""
    
    @staticmethod
    def load_config(config_file: str = "project_config.yaml") -> Dict:
        """Load configuration from YAML file"""
        try:
            with open(config_file, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            return config
        except FileNotFoundError:
            print(f"ERROR: Configuration file '{config_file}' not found!")
            sys.exit(1)
        except Exception as e:
            print(f"ERROR: Error loading configuration: {e}")
            sys.exit(1)

# ==========================================
# ENHANCED LOGGER (UTF-8 Safe)
# ==========================================
class CustomLogger:
    """Enhanced logging with UTF-8 support for all platforms"""
    
    def __init__(self, log_file: str = "waterfall_generation.log"):
        self.logger = logging.getLogger("WaterfallGenerator")
        self.logger.setLevel(logging.DEBUG)
        
        # File handler with UTF-8 encoding
        fh = logging.FileHandler(log_file, mode='w', encoding='utf-8')
        fh.setLevel(logging.DEBUG)
        
        # Console handler with UTF-8 encoding
        ch = logging.StreamHandler()
        ch.setLevel(logging.INFO)
        
        # For Windows console compatibility
        if sys.platform == 'win32':
            try:
                if hasattr(sys.stdout, 'buffer'):
                    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
                if hasattr(sys.stderr, 'buffer'):
                    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')
            except Exception:
                pass  # Already wrapped or other issue
        
        # Formatter
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S'
        )
        fh.setFormatter(formatter)
        ch.setFormatter(formatter)
        
        self.logger.addHandler(fh)
        self.logger.addHandler(ch)
    
    def get_logger(self):
        return self.logger

# ==========================================
# ADVANCED VARIABLE MANAGER
# ==========================================
class Z3VariableManager:
    """
    Advanced Z3 variable manager with type inference and normalization
    Handles complex Excel variable names and multiple naming conventions
    """
    
    def __init__(self):
        self.variables = {}
        self.var_type_map = {}
        self.original_to_normalized = {}
        self.normalized_to_original = {}
        self.variable_usage = defaultdict(set)  # Track which policies use which variables
    
    def normalize_name(self, raw_name: str) -> str:
        """
        Normalizes variable names from Excel format to Python-safe Z3 names
        Handles: 'WS.riskModel'[1].'FICO-Score-10-T', PRM3.App[1].Attr.'PIL0300'
        """
        if not isinstance(raw_name, str):
            raw_name = str(raw_name)
        
        clean = raw_name.strip()
        
        # Remove surrounding quotes
        clean = re.sub(r"^['\"]|['\"]$", "", clean)
        
        # Handle array indices: [1] -> _1_
        clean = re.sub(r"\[(\d+)\]", r"_\1_", clean)
        
        # Remove quotes around segments
        clean = re.sub(r"['\"]", "", clean)
        
        # Replace dots, hyphens, spaces with underscores
        clean = re.sub(r"[.\-\s]+", "_", clean)
        
        # Remove any remaining special characters
        clean = re.sub(r"[^a-zA-Z0-9_]", "_", clean)
        
        # Remove consecutive underscores
        clean = re.sub(r"_+", "_", clean)
        
        # Remove leading/trailing underscores
        clean = clean.strip("_")
        
        # Ensure it starts with a letter or underscore
        if clean and clean[0].isdigit():
            clean = "var_" + clean
        
        return clean if clean else "unknown_var"
    
    def infer_type(self, value: Any) -> str:
        """Infer Z3 variable type from value"""
        if pd.isna(value):
            return 'int'  # Default
        
        val_str = str(value).strip().strip('"').strip("'")
        
        # Check if numeric
        try:
            if '.' in val_str:
                float(val_str)
                return 'real'
            else:
                int(val_str)
                return 'int'
        except ValueError:
            return 'string'
    
    def get_var(self, raw_name: str, detected_type: str = 'int', policy_name: Optional[str] = None):
        """Get or create Z3 variable with proper type"""
        norm_name = self.normalize_name(raw_name)
        
        # Store mapping
        self.original_to_normalized[raw_name] = norm_name
        self.normalized_to_original[norm_name] = raw_name
        
        # Track variable usage
        if policy_name:
            self.variable_usage[norm_name].add(policy_name)
        
        if norm_name not in self.variables:
            if detected_type == 'real':
                self.variables[norm_name] = Real(norm_name)
            elif detected_type == 'string':
                self.variables[norm_name] = String(norm_name)
            else:
                self.variables[norm_name] = Int(norm_name)
            
            self.var_type_map[norm_name] = detected_type
            logger.debug(f"Created Z3 variable: {norm_name} (type: {detected_type}) from {raw_name}")
        
        return self.variables[norm_name], norm_name
    
    def get_variables_for_policy(self, policy_name: str) -> Set[str]:
        """Get all variables used by a specific policy"""
        return {var for var, policies in self.variable_usage.items() if policy_name in policies}

# ==========================================
# ADVANCED RULE PARSER
# ==========================================
class AdvancedRuleParser:
    """
    Parses complex Excel logical expressions into Z3 constraints
    Handles:
    - Chained comparisons: X >= 5 and <= 10
    - OR/AND logic
    - Multiple operators: >=, <=, !=, <>, =
    - Parentheses grouping
    - String comparisons
    """
    
    def __init__(self, var_manager: Z3VariableManager):
        self.vm = var_manager
        self.current_policy = None
    
    def set_current_policy(self, policy_name: str):
        """Set the current policy being parsed for variable tracking"""
        self.current_policy = policy_name
    
    def parse_condition(self, condition_str: Any) -> Optional[Any]:
        """Main entry point for parsing conditions"""
        if pd.isna(condition_str) or str(condition_str).strip() == "":
            return None
        
        text = str(condition_str).strip()
        logger.debug(f"Parsing condition: {text}")
        
        try:
            return self._parse_with_parentheses(text)
        except Exception as e:
            logger.error(f"Error parsing condition '{text}': {e}")
            return None
    
    def _parse_with_parentheses(self, text: str) -> Optional[Any]:
        """Handle parentheses in expressions"""
        # Remove outer parentheses if they wrap the entire expression
        text = text.strip()
        if text.startswith('(') and text.endswith(')'):
            # Check if these are matching outer parens
            depth = 0
            for i, char in enumerate(text):
                if char == '(':
                    depth += 1
                elif char == ')':
                    depth -= 1
                if depth == 0 and i < len(text) - 1:
                    break
            if i == len(text) - 1:
                text = text[1:-1].strip()
        
        # Parse OR at the top level
        return self._parse_or(text)
    
    def _parse_or(self, text: str) -> Optional[Any]:
        """Parse OR logic"""
        # Split by 'or' (case insensitive) outside of parentheses
        parts = self._split_by_operator(text, 'or')
        
        if len(parts) > 1:
            parsed_parts = [self._parse_and(p) for p in parts]
            parsed_parts = [p for p in parsed_parts if p is not None]
            if not parsed_parts:
                return None
            if len(parsed_parts) == 1:
                return parsed_parts[0]
            return Or(parsed_parts)
        
        return self._parse_and(text)
    
    def _parse_and(self, text: str) -> Optional[Any]:
        """Parse AND logic with chained comparison support"""
        parts = self._split_by_operator(text, 'and')
        
        if len(parts) > 1:
            constraints = []
            last_var_raw = None
            
            for part in parts:
                part = part.strip()
                
                # Check if this part starts with a comparison operator (chained)
                if re.match(r'^\s*(>=|<=|!=|<>|=|==|>|<)', part):
                    # Chained comparison: use variable from previous part
                    if last_var_raw:
                        cons, _ = self._parse_single_expr(part, override_var=last_var_raw)
                        if cons is not None:
                            constraints.append(cons)
                else:
                    # Full expression
                    cons, extracted_var = self._parse_single_expr(part)
                    if cons is not None:
                        constraints.append(cons)
                        if extracted_var:
                            last_var_raw = extracted_var
            
            if not constraints:
                return None
            if len(constraints) == 1:
                return constraints[0]
            return And(constraints)
        
        # Single expression
        cons, _ = self._parse_single_expr(text)
        return cons
    
    def _split_by_operator(self, text: str, operator: str) -> List[str]:
        """Split text by operator, respecting parentheses"""
        parts = []
        current = []
        depth = 0
        i = 0
        
        operator_pattern = re.compile(r'\s+' + operator + r'\s+', re.IGNORECASE)
        
        while i < len(text):
            char = text[i]
            
            if char == '(':
                depth += 1
                current.append(char)
                i += 1
            elif char == ')':
                depth -= 1
                current.append(char)
                i += 1
            elif depth == 0:
                # Check for operator
                match = operator_pattern.match(text, i)
                if match:
                    parts.append(''.join(current).strip())
                    current = []
                    i = match.end()
                else:
                    current.append(char)
                    i += 1
            else:
                current.append(char)
                i += 1
        
        if current:
            parts.append(''.join(current).strip())
        
        return parts if len(parts) > 1 else [text]
    
    def _parse_single_expr(self, text: str, override_var: Optional[str] = None) -> Tuple[Optional[Any], Optional[str]]:
        """Parse a single comparison expression"""
        text = text.strip()
        
        # Pattern for: Variable Operator Value
        pattern = r"(.+?)\s*(>=|<=|!=|<>|==|=|>|<)\s*(.+)"
        
        # If override_var is provided, check for operator-only format
        if override_var:
            op_pattern = r"^\s*(>=|<=|!=|<>|==|=|>|<)\s*(.+)"
            op_match = re.match(op_pattern, text)
            if op_match:
                op, val = op_match.groups()
                return self._build_constraint(override_var, op, val), override_var
        
        match = re.match(pattern, text)
        if not match:
            logger.warning(f"Could not parse expression: {text}")
            return None, None
        
        var_raw, op, val = match.groups()
        var_raw = var_raw.strip()
        
        return self._build_constraint(var_raw, op, val), var_raw
    
    def _build_constraint(self, var_raw: str, op: str, val: str) -> Optional[Any]:
        """Build Z3 constraint from variable, operator, and value"""
        val = val.strip().strip('"').strip("'")
        
        # Infer type and create variable
        var_type = self.vm.infer_type(val)
        z3_var, norm_name = self.vm.get_var(var_raw, var_type, self.current_policy)
        
        # Convert value to appropriate Z3 type
        try:
            if var_type == 'real':
                z3_val = float(val)
            elif var_type == 'int':
                z3_val = int(val)
            else:
                z3_val = StringVal(val)
        except ValueError:
            logger.warning(f"Could not convert value '{val}' for variable '{var_raw}'")
            return None
        
        # Build constraint
        op = op.strip()
        if op == '>':
            return z3_var > z3_val
        elif op == '<':
            return z3_var < z3_val
        elif op == '>=':
            return z3_var >= z3_val
        elif op == '<=':
            return z3_var <= z3_val
        elif op in ['=', '==']:
            return z3_var == z3_val
        elif op in ['!=', '<>']:
            return z3_var != z3_val
        else:
            logger.warning(f"Unknown operator: {op}")
            return None

# ==========================================
# POLICY DEPENDENCY GRAPH BUILDER
# ==========================================
class PolicyDependencyGraph:
    """
    Builds and analyzes dependency graph between policies
    Identifies conflicts, unreachable policies, and optimizes solving
    """
    
    def __init__(self, var_manager: Z3VariableManager):
        self.vm = var_manager
        self.graph = nx.DiGraph()
        self.policy_variables = {}  # Policy -> Set of variables
        self.variable_policies = defaultdict(set)  # Variable -> Set of policies
        self.conflict_matrix = {}  # (PolicyA, PolicyB) -> shares variables
    
    def build_graph(self, policy_map: Dict[str, Any], policy_order: List[str]):
        """Build dependency graph from policy rules"""
        logger.info("Building policy dependency graph...")
        
        # Extract variables for each policy
        for policy_name in policy_order:
            if policy_name in policy_map:
                variables = self.vm.get_variables_for_policy(policy_name)
                self.policy_variables[policy_name] = variables
                
                # Track reverse mapping
                for var in variables:
                    self.variable_policies[var].add(policy_name)
                
                # Add node to graph
                self.graph.add_node(policy_name, variables=variables)
        
        # Build edges based on shared variables
        for i, policy_a in enumerate(policy_order):
            if policy_a not in self.policy_variables:
                continue
                
            for j, policy_b in enumerate(policy_order):
                if i >= j or policy_b not in self.policy_variables:
                    continue
                
                # Check if policies share variables
                shared_vars = self.policy_variables[policy_a] & self.policy_variables[policy_b]
                
                if shared_vars:
                    # Add edge (directed: earlier policy -> later policy)
                    if i < j:
                        self.graph.add_edge(policy_a, policy_b, shared_vars=shared_vars)
                        self.conflict_matrix[(policy_a, policy_b)] = shared_vars
                    
                    logger.debug(f"Conflict edge: {policy_a} <-> {policy_b} (shared: {len(shared_vars)} vars)")
        
        logger.info(f"Graph built: {self.graph.number_of_nodes()} nodes, {self.graph.number_of_edges()} edges")
        
        # Analyze graph
        self._analyze_graph(policy_order)
    
    def _analyze_graph(self, policy_order: List[str]):
        """Analyze graph for optimization opportunities"""
        logger.info("\n" + "="*60)
        logger.info("POLICY DEPENDENCY ANALYSIS")
        logger.info("="*60)
        
        # Find isolated policies (no conflicts with others)
        isolated = [p for p in policy_order if self.graph.degree(p) == 0]
        if isolated:
            logger.info(f"[OK] Isolated policies (no conflicts): {len(isolated)}")
            for p in isolated[:5]:  # Show first 5
                logger.info(f"  - {p}")
            if len(isolated) > 5:
                logger.info(f"  ... and {len(isolated) - 5} more")
        
        # Find highly connected policies (many conflicts)
        high_degree = [(p, self.graph.degree(p)) for p in policy_order if self.graph.degree(p) > 5]
        if high_degree:
            logger.info(f"\n[WARNING] Highly connected policies (>5 conflicts):")
            for p, deg in sorted(high_degree, key=lambda x: x[1], reverse=True)[:5]:
                logger.info(f"  - {p}: {deg} conflicts")
        
        # Variable usage statistics
        var_usage = [(var, len(policies)) for var, policies in self.variable_policies.items()]
        top_vars = sorted(var_usage, key=lambda x: x[1], reverse=True)[:5]
        
        logger.info(f"\n[STATS] Most used variables:")
        for var, count in top_vars:
            logger.info(f"  - {var}: used in {count} policies")
        
        logger.info("="*60 + "\n")
    
    def get_relevant_predecessors(self, policy_name: str, all_previous: List[str]) -> List[str]:
        """
        Get only the relevant previous policies that could conflict
        This is the KEY OPTIMIZATION
        """
        if policy_name not in self.graph:
            return []
        
        relevant = []
        for prev_policy in all_previous:
            # Check if there's a path or shared variables
            if prev_policy in self.graph:
                shared_vars = self.conflict_matrix.get((prev_policy, policy_name), set())
                if shared_vars:
                    relevant.append(prev_policy)
        
        return relevant
    
    def detect_unreachable_policies(self, policy_map: Dict[str, Any], policy_order: List[str]) -> List[Tuple[str, str]]:
        """
        Detect policies that are mathematically unreachable (shadowed by earlier policies)
        Returns list of (unreachable_policy, reason)
        """
        unreachable = []
        
        for i, current_policy in enumerate(policy_order):
            if current_policy not in policy_map:
                continue
            
            # Check if current policy can be satisfied given all previous policies
            solver = Solver()
            solver.set("timeout", 5000)  # 5 second timeout for quick check
            
            # Add current policy
            solver.add(policy_map[current_policy])
            
            # Add negation of all relevant previous policies
            for j in range(i):
                prev_policy = policy_order[j]
                if prev_policy in policy_map:
                    shared_vars = self.conflict_matrix.get((prev_policy, current_policy), set())
                    if shared_vars:
                        solver.add(Not(policy_map[prev_policy]))
            
            # Check satisfiability
            result = solver.check()
            
            if result == unsat:
                # Find which previous policy shadows this one
                shadowing_policy = None
                for j in range(i-1, -1, -1):
                    prev_policy = policy_order[j]
                    if prev_policy in policy_map:
                        shared_vars = self.conflict_matrix.get((prev_policy, current_policy), set())
                        if shared_vars:
                            shadowing_policy = prev_policy
                            break
                
                reason = f"Shadowed by {shadowing_policy}" if shadowing_policy else "Conflicts with previous policies"
                unreachable.append((current_policy, reason))
                logger.warning(f"[WARNING] UNREACHABLE: {current_policy} - {reason}")
        
        return unreachable

# ==========================================
# POLICY RULE LOADER
# ==========================================
class PolicyRuleLoader:
    """
    Loads policy rules from Excel with flexible sheet structure
    Handles mid-sheet headers and different naming conventions
    """
    
    def __init__(self, var_manager: Z3VariableManager, parser: AdvancedRuleParser):
        self.vm = var_manager
        self.parser = parser
        self.policy_map = {}
    
    def load_from_excel(self, file_path: str, sheet_name: str, config: Dict):
        """Load policy rules from Excel sheet"""
        logger.info(f"Loading policy rules from {file_path}, sheet: {sheet_name}")
        
        try:
            df = pd.read_excel(file_path, sheet_name=sheet_name, header=None)
        except Exception as e:
            logger.error(f"Error loading Excel file: {e}")
            return
        
        # Detect header row(s) and parse structure
        self._parse_policy_structure(df, config)
    
    def _parse_policy_structure(self, df: pd.DataFrame, config: Dict):
        """Parse policy structure with flexible header detection"""
        policy_col_idx = config.get('policy_name_column', 0)
        rule_col_idx = config.get('rule_definition_column', 1)
        conj_col_idx = config.get('conjugation_column', 2)
        
        current_policy = None
        current_rules = []
        
        for idx, row in df.iterrows():
            # Convert row to list
            row_values = row.tolist()
            
            # Check if this is a header row (contains "Boolean Expression", "Rule Set", etc.)
            if self._is_header_row(row_values):
                logger.debug(f"Skipping header row at index {idx}")
                continue
            
            # Get policy name
            policy_name = row_values[policy_col_idx] if len(row_values) > policy_col_idx else None
            
            # Check if new policy starts
            if pd.notna(policy_name) and str(policy_name).strip() != "":
                # Save previous policy if exists
                if current_policy and current_rules:
                    self._save_policy(current_policy, current_rules)
                
                current_policy = str(policy_name).strip()
                current_rules = []
                self.parser.set_current_policy(current_policy)
                logger.debug(f"Started parsing policy: {current_policy}")
            
            # Get rule definition
            rule_def = row_values[rule_col_idx] if len(row_values) > rule_col_idx else None
            
            if pd.notna(rule_def) and str(rule_def).strip() != "":
                rule_text = str(rule_def).strip()
                conjugation = row_values[conj_col_idx] if len(row_values) > conj_col_idx else 'AND'
                conjugation = str(conjugation).upper().strip() if pd.notna(conjugation) else 'AND'
                
                # Parse rule
                z3_constraint = self.parser.parse_condition(rule_text)
                if z3_constraint is not None:
                    current_rules.append((z3_constraint, conjugation))
                    logger.debug(f"  Added rule: {rule_text[:50]}...")
        
        # Save last policy
        if current_policy and current_rules:
            self._save_policy(current_policy, current_rules)
    
    def _is_header_row(self, row_values: List) -> bool:
        """Detect if row is a header row"""
        header_keywords = [
            'boolean expression', 'rule set', 'conjugation',
            'policy name', 'rule definition', 'condition'
        ]
        
        for val in row_values:
            if pd.notna(val):
                val_str = str(val).lower().strip()
                if any(keyword in val_str for keyword in header_keywords):
                    return True
        return False
    
    def _save_policy(self, policy_name: str, rules_list: List[Tuple]):
        """Combine rules into single Z3 expression for policy"""
        if not rules_list:
            return
        
        # Start with first rule
        combined_expr = rules_list[0][0]
        
        # Combine subsequent rules based on their conjugation
        for i in range(1, len(rules_list)):
            expr, _ = rules_list[i]
            prev_conj = rules_list[i-1][1]
            
            if 'OR' in prev_conj:
                combined_expr = Or(combined_expr, expr)
            else:  # AND or N/A
                combined_expr = And(combined_expr, expr)
        
        self.policy_map[policy_name] = combined_expr
        logger.info(f"[OK] Loaded policy: {policy_name} with {len(rules_list)} rule(s)")

# ==========================================
# OPTIMIZED WATERFALL DATA GENERATOR
# ==========================================
class OptimizedWaterfallGenerator:
    """
    Generates test data using Control Flow Graph optimization
    Only negates policies that can actually conflict (share variables)
    """
    
    def __init__(self, var_manager: Z3VariableManager, policy_loader: PolicyRuleLoader, 
                 dependency_graph: PolicyDependencyGraph):
        self.vm = var_manager
        self.policy_loader = policy_loader
        self.dep_graph = dependency_graph
        self.solver = Solver()
        self.generation_stats = {
            'total_policies': 0,
            'successful': 0,
            'unsat': 0,
            'unknown': 0,
            'skipped': 0,
            'avg_constraints_added': 0,
            'total_time': 0
        }
    
    def generate(self, policy_order: List[str], template_headers: List[str]) -> pd.DataFrame:
        """Generate test data for policies in order with CFG optimization"""
        results = []
        previous_policies = {}  # Dict to store policy expressions
        total_constraints_added = 0
        
        logger.info(f"\n{'='*60}")
        logger.info(f"STARTING OPTIMIZED WATERFALL GENERATION")
        logger.info(f"{'='*60}\n")
        
        self.generation_stats['total_policies'] = len(policy_order)
        start_time = datetime.now()
        
        for idx, policy_name in enumerate(policy_order, 1):
            policy_name = str(policy_name).strip()
            
            logger.info(f"[{idx}/{len(policy_order)}] Processing: {policy_name}")
            
            if policy_name not in self.policy_loader.policy_map:
                logger.warning(f"  [WARNING] Policy '{policy_name}' not found in rules - SKIPPED")
                self.generation_stats['skipped'] += 1
                continue
            
            # Reset solver
            self.solver.reset()
            
            # Add current policy constraint (MUST satisfy)
            current_policy_expr = self.policy_loader.policy_map[policy_name]
            self.solver.add(current_policy_expr)
            logger.debug(f"  [OK] Added: MUST satisfy {policy_name}")
            
            constraints_this_round = 1
            
            # OPTIMIZATION: Only negate relevant previous policies
            relevant_previous = self.dep_graph.get_relevant_predecessors(
                policy_name, 
                list(previous_policies.keys())
            )
            
            if relevant_previous:
                logger.debug(f"  [INFO] Relevant conflicts: {len(relevant_previous)}/{len(previous_policies)} previous policies")
                
                # Add negation of only relevant previous policies
                negated_policies = [previous_policies[p] for p in relevant_previous]
                if negated_policies:
                    self.solver.add(Not(Or(negated_policies)))
                    constraints_this_round += len(relevant_previous)
                    logger.debug(f"  [OK] Added: MUST NOT satisfy {len(relevant_previous)} conflicting policies")
            else:
                logger.debug(f"  [INFO] No conflicts detected with previous policies")
            
            total_constraints_added += constraints_this_round
            
            # Solve
            logger.debug(f"  [SEARCH] Solving with {constraints_this_round} constraints...")
            check_result = self.solver.check()
            
            if check_result == sat:
                model = self.solver.model()
                row_data = self._extract_values(model, template_headers, policy_name)
                results.append(row_data)
                logger.info(f"  [OK] SAT - Solution found")
                self.generation_stats['successful'] += 1
                
                # Add to previous policies
                previous_policies[policy_name] = current_policy_expr
            
            elif check_result == unsat:
                logger.warning(f"  [FAIL] UNSAT - No solution exists")
                self.generation_stats['unsat'] += 1
                
                # Diagnose conflict
                conflicting = self._diagnose_conflict(policy_name, relevant_previous, previous_policies)
                if conflicting:
                    logger.warning(f"    Likely conflicts with: {', '.join(conflicting[:3])}")
                
                # Still add to previous policies to maintain waterfall
                previous_policies[policy_name] = current_policy_expr
            
            else:
                logger.error(f"  [FAIL] UNKNOWN - Solver timeout or resource limit")
                self.generation_stats['unknown'] += 1
                previous_policies[policy_name] = current_policy_expr
        
        # Calculate statistics
        end_time = datetime.now()
        self.generation_stats['total_time'] = (end_time - start_time).total_seconds()
        self.generation_stats['avg_constraints_added'] = (
            total_constraints_added / len(policy_order) if policy_order else 0
        )
        
        self._print_statistics()
        
        logger.info(f"\nGeneration complete. Created {len(results)} test records")
        return pd.DataFrame(results)
    
    def _diagnose_conflict(self, current_policy: str, relevant_previous: List[str], 
                          previous_policies: Dict) -> List[str]:
        """Diagnose which previous policies cause UNSAT"""
        conflicting = []
        current_expr = self.policy_loader.policy_map[current_policy]
        
        # Test each relevant previous policy individually
        for prev_policy in relevant_previous:
            test_solver = Solver()
            test_solver.set("timeout", 2000)
            
            test_solver.add(current_expr)
            test_solver.add(Not(previous_policies[prev_policy]))
            
            if test_solver.check() == unsat:
                conflicting.append(prev_policy)
        
        return conflicting
    
    def _extract_values(self, model, headers: List[str], policy_name: str) -> Dict:
        """Extract values from Z3 model and map to Excel headers"""
        row_data = {'Target_Policy': policy_name}
        
        # Get variables actually used by this policy
        policy_variables = self.vm.get_variables_for_policy(policy_name)
        
        for header in headers:
            if header == 'Target_Policy':
                continue
                
            norm_header = self.vm.normalize_name(header)
            
            value = None
            
            # Only extract value if this variable is used by the current policy
            if norm_header in policy_variables:
                if norm_header in self.vm.variables:
                    z3_var = self.vm.variables[norm_header]
                    z3_val = model.eval(z3_var, model_completion=True)
                    
                    if z3_val is not None:
                        try:
                            if is_int(z3_val):
                                value = z3_val.as_long()
                            elif is_real(z3_val):
                                value = float(z3_val.as_decimal(10).replace('?', ''))
                            elif is_string_value(z3_val):
                                value = str(z3_val).strip('"')
                            else:
                                value = str(z3_val)
                        except Exception as e:
                            logger.warning(f"Error extracting value for {header}: {e}")
                            value = None
            
            # Leave as None (blank in Excel) if not used by this policy
            row_data[header] = value
        
        return row_data
    
    def _print_statistics(self):
        """Print generation statistics"""
        stats = self.generation_stats
        
        logger.info(f"\n{'='*60}")
        logger.info(f"GENERATION STATISTICS")
        logger.info(f"{'='*60}")
        logger.info(f"Total Policies:      {stats['total_policies']}")
        logger.info(f"Successful (SAT):    {stats['successful']} ({stats['successful']/stats['total_policies']*100:.1f}%)")
        logger.info(f"Failed (UNSAT):      {stats['unsat']} ({stats['unsat']/stats['total_policies']*100:.1f}%)")
        logger.info(f"Unknown:             {stats['unknown']}")
        logger.info(f"Skipped:             {stats['skipped']}")
        logger.info(f"Avg Constraints:     {stats['avg_constraints_added']:.1f} per policy")
        logger.info(f"Total Time:          {stats['total_time']:.2f} seconds")
        logger.info(f"{'='*60}\n")

# ==========================================
# MINIMUM SET COVER OPTIMIZER
# ==========================================
class MinimumSetCoverOptimizer:
    """
    Calculates minimum number of test cases needed to cover all policies
    Uses greedy set cover algorithm
    """
    
    def __init__(self, dependency_graph: PolicyDependencyGraph):
        self.dep_graph = dependency_graph
    
    def calculate_minimum_coverage(self, policy_order: List[str]) -> List[List[str]]:
        """
        Calculate minimum test cases to cover all policies
        Returns list of policy groups that can share test cases
        """
        logger.info("\n" + "="*60)
        logger.info("MINIMUM SET COVER ANALYSIS")
        logger.info("="*60)
        
        # Group policies by conflict clusters
        clusters = []
        remaining_policies = set(policy_order)
        
        while remaining_policies:
            # Start new cluster with first remaining policy
            seed = remaining_policies.pop()
            cluster = {seed}
            
            # Add all policies that don't conflict with anything in cluster
            to_check = list(remaining_policies)
            for policy in to_check:
                conflicts = False
                for cluster_policy in cluster:
                    if self.dep_graph.conflict_matrix.get((cluster_policy, policy)) or \
                       self.dep_graph.conflict_matrix.get((policy, cluster_policy)):
                        conflicts = True
                        break
                
                if not conflicts:
                    cluster.add(policy)
                    remaining_policies.remove(policy)
            
            clusters.append(list(cluster))
        
        logger.info(f"[OK] Minimum test cases needed: {len(clusters)} (vs {len(policy_order)} policies)")
        logger.info(f"  Reduction: {(1 - len(clusters)/len(policy_order))*100:.1f}%")
        
        for i, cluster in enumerate(clusters[:5], 1):
            logger.info(f"  Cluster {i}: {len(cluster)} policies")
        
        if len(clusters) > 5:
            logger.info(f"  ... and {len(clusters) - 5} more clusters")
        
        logger.info("="*60 + "\n")
        
        return clusters

# ==========================================
# MAIN ORCHESTRATOR
# ==========================================
class WaterfallOrchestrator:
    """Main orchestrator for the entire generation process"""
    
    def __init__(self, config_file: str = "project_config.yaml"):
        self.config = ConfigLoader.load_config(config_file)
        self.vm = Z3VariableManager()
        self.parser = AdvancedRuleParser(self.vm)
        self.policy_loader = PolicyRuleLoader(self.vm, self.parser)
        self.dep_graph = None
        self.generator = None
        self.min_cover_optimizer = None
    
    def run(self):
        """Execute the complete waterfall generation process"""
        logger.info("="*60)
        logger.info("ADVANCED WATERFALL GENERATOR WITH CFG OPTIMIZATION")
        logger.info("="*60)
        
        # Load configuration
        project = self.config['project']
        logger.info(f"Project: {project['name']}")
        logger.info(f"Version: {project.get('version', '1.0')}")
        
        # Load policy rules
        rules_file = project['rules_file']
        rules_sheet = project['rules_sheet']
        self.policy_loader.load_from_excel(rules_file, rules_sheet, project)
        
        logger.info(f"\n[OK] Loaded {len(self.policy_loader.policy_map)} policies")
        
        # Load policy order
        order_file = project['order_file']
        order_sheet = project['order_sheet']
        order_column = project.get('order_column', 0)
        
        df_order = pd.read_excel(order_file, sheet_name=order_sheet)
        policy_order = df_order.iloc[:, order_column].dropna().tolist()
        policy_order = [str(p).strip() for p in policy_order]
        logger.info(f"[OK] Loaded execution order: {len(policy_order)} policies")
        
        # Load template headers
        template_file = project['template_file']
        template_sheet = project.get('template_sheet', 0)
        df_template = pd.read_excel(template_file, sheet_name=template_sheet)
        headers = df_template.columns.tolist()
        
        # Add Target_Policy column if not present
        if 'Target_Policy' not in headers:
            headers = ['Target_Policy'] + headers
        
        logger.info(f"[OK] Loaded template: {len(headers)} attributes")
        
        # Build dependency graph
        self.dep_graph = PolicyDependencyGraph(self.vm)
        self.dep_graph.build_graph(self.policy_loader.policy_map, policy_order)
        
        # Detect unreachable policies
        unreachable = self.dep_graph.detect_unreachable_policies(
            self.policy_loader.policy_map, 
            policy_order
        )
        
        if unreachable:
            logger.warning(f"\n[WARNING] {len(unreachable)} unreachable policies detected!")
            logger.warning("These policies cannot be satisfied due to conflicts:")
            for policy, reason in unreachable[:10]:
                logger.warning(f"  - {policy}: {reason}")
            if len(unreachable) > 10:
                logger.warning(f"  ... and {len(unreachable) - 10} more")
            logger.warning("")
        
        # Calculate minimum set cover (optional analysis)
        self.min_cover_optimizer = MinimumSetCoverOptimizer(self.dep_graph)
        self.min_cover_optimizer.calculate_minimum_coverage(policy_order)
        
        # Generate data with optimization
        self.generator = OptimizedWaterfallGenerator(
            self.vm, 
            self.policy_loader,
            self.dep_graph
        )
        result_df = self.generator.generate(policy_order, headers)
        
        # Save output
        output_file = project['output_file']
        result_df.to_excel(output_file, index=False)
        logger.info(f"[OK] Output saved to: {output_file}")
        
        # Generate summary report
        self._generate_summary_report(output_file, result_df, unreachable)
        
        logger.info("\n" + "="*60)
        logger.info("GENERATION COMPLETE")
        logger.info("="*60)
    
    def _generate_summary_report(self, output_file: str, result_df: pd.DataFrame, 
                                 unreachable: List[Tuple[str, str]]):
        """Generate a summary report"""
        report_file = output_file.replace('.xlsx', '_REPORT.txt')
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("="*80 + "\n")
            f.write("WATERFALL TEST DATA GENERATION REPORT\n")
            f.write("="*80 + "\n\n")
            
            f.write(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"Project: {self.config['project']['name']}\n\n")
            
            f.write("SUMMARY:\n")
            f.write("-"*80 + "\n")
            f.write(f"Total Policies Loaded:        {len(self.policy_loader.policy_map)}\n")
            f.write(f"Total Test Cases Generated:   {len(result_df)}\n")
            f.write(f"Successful (SAT):             {self.generator.generation_stats['successful']}\n")
            f.write(f"Failed (UNSAT):               {self.generator.generation_stats['unsat']}\n")
            f.write(f"Unknown:                      {self.generator.generation_stats['unknown']}\n")
            f.write(f"Skipped:                      {self.generator.generation_stats['skipped']}\n")
            f.write(f"Average Constraints per Solve: {self.generator.generation_stats['avg_constraints_added']:.1f}\n")
            f.write(f"Total Generation Time:        {self.generator.generation_stats['total_time']:.2f}s\n\n")
            
            if unreachable:
                f.write("UNREACHABLE POLICIES:\n")
                f.write("-"*80 + "\n")
                f.write(f"Total: {len(unreachable)}\n\n")
                for policy, reason in unreachable:
                    f.write(f"  - {policy}\n")
                    f.write(f"    Reason: {reason}\n\n")
            
            f.write("\nGRAPH STATISTICS:\n")
            f.write("-"*80 + "\n")
            f.write(f"Total Nodes (Policies):       {self.dep_graph.graph.number_of_nodes()}\n")
            f.write(f"Total Edges (Conflicts):      {self.dep_graph.graph.number_of_edges()}\n")
            f.write(f"Average Conflicts per Policy: {self.dep_graph.graph.number_of_edges() / max(1, self.dep_graph.graph.number_of_nodes()):.1f}\n")
            
            f.write("\n" + "="*80 + "\n")
        
        logger.info(f"[OK] Summary report saved to: {report_file}")

# ==========================================
# ENTRY POINT
# ==========================================
if __name__ == "__main__":
    # Initialize logger
    custom_logger = CustomLogger()
    logger = custom_logger.get_logger()
    
    # Check for config file argument
    config_file = sys.argv[1] if len(sys.argv) > 1 else "project_config.yaml"
    
    try:
        orchestrator = WaterfallOrchestrator(config_file)
        orchestrator.run()
    except Exception as e:
        logger.error(f"Fatal error: {e}", exc_info=True)
        sys.exit(1)


# ==========================================
# WATERFALL TEST DATA GENERATOR WITH CFG
# PROJECT CONFIGURATION FILE
# ==========================================
# 
# This configuration file supports the advanced Control Flow Graph (CFG)
# optimization that dramatically improves performance for large rule sets.
#
# Usage: python waterfall_generator_cfg.py project_config.yaml
# ==========================================

project:
  # Project identification
  name: "Credit Card Application - CFG Optimized"
  version: "2.0"
  description: "Credit card policy rules with waterfall logic and CFG optimization"
  
  # ==========================================
  # FILE PATHS
  # ==========================================
  
  # Rules file (contains policy definitions and conditions)
  rules_file: "PolicyRules.xlsx"
  rules_sheet: "CC Policy Rules Set"  # Exact sheet name
  
  # Order file (contains the sequence of policies to process)
  order_file: "PolicyRules.xlsx"  # Can be same or different file
  order_sheet: "PolicyOrder"
  
  # Template file (contains the attribute headers to populate)
  template_file: "InputTemplate.xlsx"
  template_sheet: 0  # Sheet index (0 = first sheet) or name
  
  # Output file (where generated test data will be saved)
  output_file: "Test_Data_Generated_CFG.xlsx"
  
  # ==========================================
  # COLUMN MAPPINGS
  # ==========================================
  # Define which columns contain what information in your rules sheet
  
  # Policy rules structure
  policy_name_column: 0      # Column index for policy names (0 = first column)
  rule_definition_column: 1  # Column index for rule definitions
  conjugation_column: 2      # Column index for AND/OR/N/A conjugations
  
  # Policy order structure  
  order_column: 0  # Column index containing the policy execution order
  
  # ==========================================
  # PARSING OPTIONS
  # ==========================================
  
  # Skip rows (if your Excel has title rows before data)
  skip_header_rows: 0
  
  # Variable naming convention hints (for better normalization)
  variable_patterns:
    - "'WS.riskModel'[1].'FICO-Score-10-T'"
    - "PRM3.App[1].Attr.'PIL0300'"
    - "('WS.riskModel'[1].'Bankruptcy-Plus-BP'"
  
  # ==========================================
  # CFG OPTIMIZATION OPTIONS
  # ==========================================
  cfg_optimization:
    enabled: true
    
    # Conflict detection
    detect_unreachable: true           # Detect policies that can never be satisfied
    warn_unreachable: true             # Show warnings for unreachable policies
    skip_unreachable: false            # Skip generating data for unreachable policies
    
    # Graph analysis
    analyze_dependencies: true         # Perform dependency graph analysis
    show_graph_stats: true            # Display graph statistics
    identify_bottlenecks: true        # Identify highly connected policies
    
    # Minimum set cover
    calculate_min_cover: true         # Calculate minimum test cases needed
    suggest_optimization: true        # Suggest ways to reduce test cases
  
  # ==========================================
  # SOLVER OPTIONS
  # ==========================================
  solver:
    timeout: 30000              # Timeout in milliseconds (30 seconds)
    max_attempts: 3             # Max retry attempts for UNKNOWN results
    conflict_diagnosis: true    # Diagnose which policies cause UNSAT
    quick_check_timeout: 5000   # Timeout for unreachable detection (5 seconds)

# ==========================================
# ADVANCED OPTIONS
# ==========================================

advanced:
  # Enable debug mode for detailed logging
  debug_mode: false
  
  # Generate additional columns in output
  include_metadata: true
  metadata_columns:
    - "Generated_Timestamp"
    - "Solver_Status"
    - "Conflicts_Resolved"
  
  # Validation rules
  validate_output: true
  validation:
    check_null_values: true
    check_data_types: true
    check_ranges: false
    verify_policy_satisfaction: true   # Verify each row satisfies its target policy
  
  # Performance options
  cache_results: true
  incremental_solving: true            # Use incremental Z3 solving
  parallel_analysis: false             # Future feature: parallel graph analysis
  
  # Output options
  generate_report: true                # Generate detailed summary report
  include_diagnostics: true            # Include diagnostic information
  export_graph: false                  # Export dependency graph (requires graphviz)

# ==========================================
# LOGGING OPTIONS
# ==========================================

logging:
  level: "INFO"  # DEBUG, INFO, WARNING, ERROR
  file: "waterfall_generation_cfg.log"
  console_output: true
  detailed_trace: false
  show_progress: true
  log_constraints: false              # Log Z3 constraints (very verbose)

# ==========================================
# REPORTING OPTIONS
# ==========================================

reporting:
  generate_summary: true
  summary_file: "auto"                # "auto" generates filename from output_file
  
  include_sections:
    - generation_statistics
    - unreachable_policies
    - graph_analysis
    - variable_usage
    - conflict_matrix
    - optimization_suggestions
  
  format: "text"                      # "text", "html", "json"


"""
Test script to verify that unused columns are truly blank (None/NaN) in output
Run this after generating your Excel file

Usage:
    python test_blank_values.py Test_Data_Generated_CFG.xlsx
"""
import pandas as pd
import sys
import os

def test_blank_values(excel_file):
    """Test if unused columns are blank"""
    print("="*60)
    print("TESTING BLANK VALUES IN GENERATED OUTPUT")
    print("="*60)
    
    # Check if file exists
    if not os.path.exists(excel_file):
        print(f"\n[ERROR] File not found: {excel_file}")
        return False
    
    # Read the generated Excel file
    try:
        df = pd.read_excel(excel_file)
    except Exception as e:
        print(f"\n[ERROR] Failed to read Excel file: {e}")
        return False
    
    print(f"\nFile: {excel_file}")
    print(f"Total rows: {len(df)}")
    print(f"Total columns: {len(df.columns)}")
    
    # Check for each column
    print("\n" + "-"*60)
    print("COLUMN ANALYSIS:")
    print("-"*60)
    
    results = []
    for col in df.columns:
        if col == 'Target_Policy':
            continue
            
        null_count = df[col].isna().sum()
        zero_count = (df[col] == 0).sum()
        non_null_count = df[col].notna().sum()
        
        # Count actual values (not null, not zero)
        actual_values = ((df[col].notna()) & (df[col] != 0)).sum()
        
        results.append({
            'Column': col,
            'Null/Blank': null_count,
            'Zeros': zero_count,
            'Has Values': non_null_count,
            'Actual Values': actual_values
        })
    
    # Display results
    result_df = pd.DataFrame(results)
    print(result_df.to_string(index=False))
    
    # Summary
    print("\n" + "="*60)
    print("SUMMARY:")
    print("="*60)
    
    totally_blank = result_df[result_df['Has Values'] == 0]
    partially_used = result_df[(result_df['Has Values'] > 0) & (result_df['Null/Blank'] > 0)]
    fully_used = result_df[result_df['Null/Blank'] == 0]
    
    print(f"Completely unused columns (all blank): {len(totally_blank)}")
    print(f"Partially used columns (some blank):   {len(partially_used)}")
    print(f"Fully used columns (no blanks):        {len(fully_used)}")
    
    # Check if any zeros where there should be blanks
    # A column should have zeros ONLY if those zeros are actual values (not placeholders)
    problem_cols = result_df[
        (result_df['Zeros'] > 0) & 
        (result_df['Null/Blank'] > 0) & 
        (result_df['Zeros'] > result_df['Actual Values'])
    ]
    
    if len(problem_cols) > 0:
        print("\n" + "="*60)
        print("[ERROR] FOUND ZEROS INSTEAD OF BLANKS!")
        print("="*60)
        print("These columns have zeros where they should be blank:")
        for _, row in problem_cols.iterrows():
            print(f"  - {row['Column']}: {row['Zeros']} zeros, {row['Null/Blank']} blanks")
        print("\nThe code needs to be fixed!")
        return False
    else:
        print("\n[OK] All unused cells are properly blank (not zeros)")
        
        # Additional check: show examples of partial usage
        if len(partially_used) > 0:
            print(f"\n[INFO] {len(partially_used)} columns are partially used:")
            for _, row in partially_used.head(5).iterrows():
                print(f"  - {row['Column']}: {row['Actual Values']} values, {row['Null/Blank']} blanks")
            if len(partially_used) > 5:
                print(f"  ... and {len(partially_used) - 5} more")
        
        return True

def detailed_analysis(excel_file):
    """Perform detailed analysis of blank patterns"""
    print("\n" + "="*60)
    print("DETAILED ANALYSIS")
    print("="*60)
    
    df = pd.read_excel(excel_file)
    
    # Analyze by policy
    if 'Target_Policy' in df.columns:
        print("\nPer-Policy Analysis (first 5 policies):")
        print("-"*60)
        
        for i, (idx, row) in enumerate(df.head(5).iterrows()):
            policy = row['Target_Policy']
            non_null = row.notna().sum() - 1  # Exclude Target_Policy
            total_cols = len(df.columns) - 1
            
            print(f"\nPolicy: {policy}")
            print(f"  Columns with values: {non_null}/{total_cols}")
            print(f"  Blank columns: {total_cols - non_null}")
            
            # Show which columns have values
            cols_with_values = [col for col in df.columns 
                              if col != 'Target_Policy' and pd.notna(row[col]) and row[col] != 0]
            if cols_with_values:
                print(f"  Variables used: {', '.join(cols_with_values[:5])}")
                if len(cols_with_values) > 5:
                    print(f"    ... and {len(cols_with_values) - 5} more")

def main():
    """Main function"""
    if len(sys.argv) < 2:
        print("="*60)
        print("Blank Values Test Script")
        print("="*60)
        print("\nUsage:")
        print("  python test_blank_values.py <your_output_file.xlsx>")
        print("\nExample:")
        print("  python test_blank_values.py Test_Data_Generated_CFG.xlsx")
        print("\nThis script verifies that:")
        print("  1. Unused columns are blank (not 0)")
        print("  2. Partially used columns are blank for rows that don't use them")
        print("  3. Only actual data values are populated")
        print("="*60)
        sys.exit(1)
    
    excel_file = sys.argv[1]
    
    try:
        # Run basic test
        success = test_blank_values(excel_file)
        
        # Run detailed analysis if basic test passes
        if success:
            detailed_analysis(excel_file)
            print("\n" + "="*60)
            print("[SUCCESS] All tests passed!")
            print("="*60)
        else:
            print("\n" + "="*60)
            print("[FAILED] Issues found - see above")
            print("="*60)
        
        sys.exit(0 if success else 1)
        
    except FileNotFoundError:
        print(f"\n[ERROR] File not found: {excel_file}")
        print("Make sure you've generated the output file first.")
        sys.exit(1)
    except Exception as e:
        print(f"\n[ERROR] Unexpected error: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()
